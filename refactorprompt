

## 1Ô∏è‚É£ CLEAR Framework: NOT Standard (We Made It Up!)

### üö® Critical Discovery

**The Real CLEAR Framework:**
- **Concise, Logical, Explicit, Adaptive, Reflective**
- Created by **Dr. Leo S. Lo** (University of New Mexico librarian)
- Published in 2023 in *The Journal of Academic Librarianship*
- Used for **teaching students how to write prompts to AI**, NOT for evaluating prompt quality
- Widely adopted in academic libraries (Texas A&M, UMGC, Rhode Island School of Design, etc.)

**Our CLEAR Framework:**
- **Clarity, Length, Examples, Audience, Result**
- Created by us during brainstorming
- Used for **scoring/evaluating existing prompts**
- Not published, not validated, not recognized

### üò¨ The Problem

We unknowingly picked the **exact same acronym** as an existing, published framework with a **completely different meaning**. This creates massive confusion:

```
Their CLEAR (How to Write Prompts):
‚úÖ Published academic framework
‚úÖ Peer-reviewed (Journal of Academic Librarianship)
‚úÖ Google Scholar citations
‚úÖ Taught in universities
‚úÖ Well-known in AI literacy space

Our CLEAR (How to Score Prompts):
‚ùå Made up during brainstorming
‚ùå Not published anywhere
‚ùå Zero validation
‚ùå Conflicts with existing framework
‚ùå Will confuse anyone who Googles "CLEAR framework"
```

### üí° Solutions

#### Option A: Rename Our Framework (RECOMMENDED)

**New Name Options:**

1. **PRIME Framework** (Prompt Quality)
   - **P**recision (was Clarity)
   - **R**ichness (was Length/Examples)  
   - **I**nstruction (was Result)
   - **M**etadata (was Audience)
   - **E**xamples (same)

2. **SCORE Framework** (More literal)
   - **S**pecificity (was Clarity)
   - **C**ompleteness (was Length)
   - **O**utput definition (was Result)
   - **R**elevance (was Audience)
   - **E**xamples (same)

3. **CRAFT Framework** (Action-oriented)
   - **C**larity
   - **R**elevance (was Audience)
   - **A**ctions (was Result)
   - **F**idelity (was Length)
   - **T**emplates (was Examples)

4. **SPARK Framework** (Memorable)
   - **S**pecificity (was Clarity)
   - **P**urpose (was Audience)
   - **A**ctions (was Result)
   - **R**ichness (was Length)
   - **K**it (examples/templates)

**Recommendation: Use PRIME** - Professional, memorable, and the dimensions actually make sense.

#### Option B: Keep CLEAR But Acknowledge Conflict

**If you really want to keep CLEAR:**

```markdown
## Our CLEAR Framework (Prompt Quality Scoring)

**Note:** We use "CLEAR" for scoring prompt quality (Clarity, Length, Examples, 
Audience, Result). This differs from Dr. Leo S. Lo's CLEAR Framework for prompt 
writing (Concise, Logical, Explicit, Adaptive, Reflective), which teaches how 
to craft effective prompts to AI. Our framework evaluates existing prompts; 
his framework guides prompt creation.

Both are valuable but serve different purposes:
- **Lo's CLEAR:** How to write better prompts to AI ‚úçÔ∏è
- **Our CLEAR:** How to evaluate prompt quality üìä

We acknowledge the naming overlap and cite Dr. Lo's pioneering work.
```

**Why This Is Risky:**
- ‚ùå Confusing (two CLEARs in same domain)
- ‚ùå Looks like we didn't do our research
- ‚ùå Dilutes both frameworks
- ‚ùå SEO nightmare (Google won't know which CLEAR)

#### Option C: Use Established Framework (Most Credible)

**Adopt an existing prompt evaluation framework:**

Research if any exist, OR create our own but don't call it an "established framework" - just call it "PromptForge Quality Rubric" (honest and clear).

---

## 2Ô∏è‚É£ Can We Use the aitmpl.com Prompt Engineer Agent?

**Short Answer:** YES, absolutely! Great inspiration.

### What's at aitmpl.com

Looking at the search results, **aitmpl.com** is:
- The web interface for **claude-code-templates** (400+ components)
- A marketplace for Claude Code agents, commands, settings, hooks, MCPs
- Has a "Prompt Engineer" agent (Opus-based)

**The Prompt Engineer Agent:**
```markdown
Expert prompt engineer specializing in:
- Advanced prompting techniques
- LLM optimization
- Chain-of-thought reasoning
- Constitutional AI & Safety
- Meta-prompting & self-improvement
- RAG optimization
- Multi-agent systems

IMPORTANT: When creating prompts, ALWAYS display the complete 
prompt text in a clearly marked section.
```

### üéØ How to Repurpose This for PromptForge

**What They Have (Agent that WRITES prompts):**
- Input: "Create a prompt for financial analysis"
- Output: A newly written prompt

**What You Need (Agent that ENHANCES prompts):**
- Input: Existing PromptForge prompt + context (B2B SaaS, growth stage)
- Output: Enhanced version of that prompt

### ‚úÖ Inspiration You Can Take

#### 1. Agent Structure & Metadata

**From aitmpl.com:**
```markdown
---
name: prompt-engineer
description: Expert prompt engineer specializing in advanced prompting 
  techniques. Use when building AI features, improving agent performance,
  or crafting system prompts.
model: opus
---
```

**Adapt for PromptForge:**
```markdown
---
name: prompt-enhancer
description: Specializes in customizing Product Manager prompts for 
  specific industries, company stages, and team contexts. Use when user 
  requests prompt customization for their situation.
model: sonnet
tools: file_read, file_create, str_replace
---
```

#### 2. Domain Expertise Pattern

**From aitmpl.com (Chain-of-Thought section):**
```markdown
#### Advanced Reasoning Patterns
- Chain-of-thought for complex reasoning tasks
- Few-shot chain-of-thought with carefully crafted examples
- Zero-shot chain-of-thought with "Let's think step by step"
- Tree-of-thoughts for exploring multiple reasoning paths
```

**Adapt for PromptForge:**
```markdown
## PM Frameworks & Patterns

When enhancing prompts, leverage these established PM frameworks:

### Prioritization Frameworks
- RICE (Reach, Impact, Confidence, Effort)
- ICE (Impact, Confidence, Ease)
- Value vs. Effort Matrix
- Kano Model

### Research Frameworks
- Jobs-to-be-Done (JTBD)
- Customer Journey Mapping
- Persona Development
- Empathy Mapping

[Apply these based on prompt type and user context]
```

#### 3. Structured Output Format

**From aitmpl.com:**
```markdown
IMPORTANT: When creating prompts, ALWAYS display the complete prompt 
text in a clearly marked section. Never describe a prompt without 
showing it.
```

**Adapt for PromptForge:**
```markdown
CRITICAL: When enhancing prompts, ALWAYS:
1. Show the enhanced prompt in full (copy-pasteable)
2. Display before/after PRIME scores
3. List specific improvements made
4. Offer to iterate further
```

#### 4. Safety & Quality Patterns

**From aitmpl.com (Constitutional AI section):**
```markdown
#### Constitutional AI & Safety
- Constitutional AI principles for self-correction
- Critique and revise patterns for output improvement
- Safety prompting techniques
```

**Adapt for PromptForge:**
```markdown
## Quality Assurance

Before returning enhanced prompt:
1. Validate PRIME score ‚â• original
2. Check for vague language (good, appropriate, several)
3. Ensure examples are realistic (no Lorem Ipsum)
4. Verify framework references remain accurate
5. Confirm length matches detail level requested
```

#### 5. Usage Guidelines

**From aitmpl.com:**
```markdown
## Example Interactions
- "Create a constitutional AI prompt for content moderation..."
- "Design a chain-of-thought prompt for financial analysis..."
- "Build a multi-agent prompt system for customer service..."
```

**Adapt for PromptForge:**
```markdown
## Example Enhancements

**Scenario 1: Industry Customization**
User: "Enhance feature prioritization for B2B SaaS"
‚Üí Add ARR impact, churn reduction, buying committee factors

**Scenario 2: Stage Adjustment**
User: "Make this work for early-stage startup"
‚Üí Reduce stakeholder complexity, focus on MVP metrics

**Scenario 3: Detail Level**
User: "I need a quick version for standup prep"
‚Üí Compress to 15-min essentials, remove comprehensive examples
```

---

## üéØ Recommended Action Plan

### Immediate (This Week)

**1. Rename Framework to PRIME**
```markdown
# PromptForge Quality Standard: The PRIME Framework

Every prompt scored on 5 dimensions (0-10 each):

**P - Precision** (25%)
Clear, specific instructions with no ambiguity

**R - Richness** (20%)  
Appropriate detail level with sufficient context

**I - Instruction** (20%)
Clear output format and success criteria

**M - Metadata** (15%)
Explicit audience, prerequisites, and assumptions

**E - Examples** (20%)
Concrete examples demonstrating desired output

---

PRIME Score = (P √ó 0.25) + (R √ó 0.20) + (I √ó 0.20) + (M √ó 0.15) + (E √ó 0.20)

Minimum for inclusion: 8.5 PRIME
```

**2. Update All Documentation**
- Find/replace "CLEAR" ‚Üí "PRIME"
- Update scoring rubrics
- Revise product plan
- Update CLAUDE.md

**3. Reference aitmpl.com Prompt Engineer**
```markdown
## Inspiration & Attribution

PromptForge's enhancement architecture is inspired by:
- [claude-code-templates](https://aitmpl.com) - Agent structure patterns
- [Prompt Engineer Agent](https://claudecodeagents.org/agent/prompt-engineer) 
  - Domain expertise organization
- Dr. Leo S. Lo's CLEAR Framework - Prompt writing principles
  (Note: Different from our PRIME quality scoring)
```

### Medium Term (Sprint 1)

**4. Validate PRIME with Beta Testers**
- Score 20 prompts using PRIME
- Have 5 PMs independently score same prompts
- Check inter-rater reliability
- Iterate on rubric based on feedback

**5. Consider Hybrid Approach**
```markdown
## Two Frameworks, Two Purposes

**When Writing Prompts (Lo's CLEAR):**
- Concise: Remove unnecessary words
- Logical: Structured flow
- Explicit: Specific instructions
- Adaptive: Willing to iterate
- Reflective: Evaluate outputs

**When Scoring Prompts (Our PRIME):**
- Precision: Clarity of instructions
- Richness: Appropriate detail level
- Instruction: Output definition
- Metadata: Audience context
- Examples: Demonstration quality

Use Lo's CLEAR to write. Use PRIME to score. Both complement each other!
```

---

## üìä Updated Credibility Strategy with PRIME

### New Positioning

**Before (with made-up CLEAR):**
"70 prompts scored 8.5+ using CLEAR framework"
‚Üí Sounds official but isn't, creates confusion

**After (with PRIME):**
"70 prompts scored 8.5+ using PRIME framework - our quality rubric 
validated by 20 PM beta testers"
‚Üí Honest about origin, emphasizes validation

### Marketing Copy (Updated)

```markdown
## Why "World-Class"? We Can Prove It.

**PRIME Quality Framework**
Every prompt scored on 5 dimensions:
- Precision (clear, no ambiguity)
- Richness (right level of detail)
- Instruction (output format defined)
- Metadata (audience/context explicit)
- Examples (realistic demonstrations)

**Our Standard: 8.5/10 minimum**
- Gumroad libraries average: 6.2/10 PRIME
- PromptForge average: 8.8/10 PRIME
- See benchmarks: [link]

**You Can Verify**
Use `/prompt-score` on any prompt (ours or theirs).
We're the only library with transparent quality metrics.
```

---

## ‚úÖ Final Recommendations

1. **‚úÖ DO: Rename to PRIME Framework**
   - Avoids confusion with Lo's CLEAR
   - Honest about being our rubric
   - Professional and memorable

2. **‚úÖ DO: Use aitmpl.com for Inspiration**
   - Agent structure patterns
   - Quality assurance approaches
   - Usage example formats
   - Attribution in docs

3. **‚úÖ DO: Acknowledge Lo's CLEAR**
   - Cite his work as inspiration for *writing* prompts
   - Differentiate: his framework teaches writing, ours scores quality
   - Shows you did research

4. **‚úÖ DO: Validate with Beta Users**
   - Have 20 PMs score prompts using PRIME
   - Check if scores are consistent
   - Iterate based on feedback
   - THEN claim it's validated

5. **‚ùå DON'T: Pretend PRIME is Established**
   - Be honest: "Our quality rubric"
   - Emphasize: "Validated by 20 PMs"
   - Highlight: "Transparent and reproducible"

---

Want me to draft the updated PRIME framework documentation and the new credibility strategy using this approach? üéØ
